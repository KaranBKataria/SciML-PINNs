{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d35bf9",
   "metadata": {},
   "source": [
    "# Investigating Failure Modes and Enhancements of Physics-Informed Neural Networks for Power System Dynamics\n",
    "\n",
    "## Vanilla NNs Numerical Experiments\n",
    "\n",
    "#### Karan Kataria\n",
    "#### Supervisor: Dr. Subhash Lakshminarayana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add ODE_numerical_solver module to PATH variable\n",
    "sys.path.insert(\n",
    "    1,\n",
    "    \"/Users/karankataria/Library/CloudStorage/OneDrive-UniversityofWarwick/PINNs_PSs/src/numerical_experiments\"\n",
    ")\n",
    "\n",
    "sys.path.insert(\n",
    "    2,\n",
    "    \"/Users/karankataria/Library/CloudStorage/OneDrive-UniversityofWarwick/PINNs_PSs/src/numerical_solutions\"\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import scienceplots\n",
    "from torchinfo import summary\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from pyhessian import hessian\n",
    "\n",
    "from utils import set_global_seed\n",
    "from pinn_architecture import PINN\n",
    "from global_constants import *\n",
    "from ODE_numerical_solver import swing_ODEs_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056e544",
   "metadata": {},
   "source": [
    "## 1.0) Set up script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7706915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config matplotlib and define plot constants\n",
    "plt.style.use(\"science\")\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "\n",
    "# Color map for the visualisations\n",
    "CMAP: str = \"seismic\" \n",
    "\n",
    "# Device to perform tensor operations on (GPU to CPU)\n",
    "\n",
    "# NB: GPUs were not needed; MPS was avaliable on my Mac but did not showcase significant\n",
    "# computational benefits\n",
    "DEVICE: str = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "834f3f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set: 20\n"
     ]
    }
   ],
   "source": [
    "# Define and fix seed\n",
    "SEED: int = 20\n",
    "set_global_seed(SEED)\n",
    "\n",
    "assert type(SEED) is int\n",
    "\n",
    "print(f\"Seed set: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ab78e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: mechanical_power\n"
     ]
    }
   ],
   "source": [
    "# Specify the parameter to be varied and its symbol\n",
    "VARYING_PARAM: str = \"mechanical_power\"\n",
    "VARYING_PARAM_NOTATION: str = \"$P^m$\"\n",
    "print(f\"Parameter: {VARYING_PARAM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a58fdc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Export Path: /Users/karankataria/Library/CloudStorage/OneDrive-UniversityofWarwick/PINNs_PSs/data/visualisations/loss_landscapes/mechanical_power/\n"
     ]
    }
   ],
   "source": [
    "# Path to save images and flag\n",
    "PATH_TO_IM_DIR: str = \"/Users/karankataria/Library/CloudStorage/OneDrive-UniversityofWarwick\"\\\n",
    "                    f\"/PINNs_PSs/data/visualisations/loss_landscapes/{VARYING_PARAM}/\"\n",
    "\n",
    "print(f\"Absolute Export Path: {PATH_TO_IM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe44a0",
   "metadata": {},
   "source": [
    "### 1.1) Set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b808ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots and files are exported from this file?\tFalse\n",
      "Semi-supervised learning enabled?\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Boolean constant only adds title on subplots on the top most row (first random seed 0)\n",
    "TOP_PLOT: bool = True\n",
    "\n",
    "# Toggle flag on whether or not to export data and images\n",
    "SAVE: bool = False\n",
    "\n",
    "SEMI_SUPERVISED: bool = False\n",
    "\n",
    "print(f\"Plots and files are exported from this file?\\t{SAVE}\")\n",
    "print(f\"Semi-supervised learning enabled?\\t{SEMI_SUPERVISED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9439278",
   "metadata": {},
   "source": [
    "### 1.2) Initialise SMIB swing equation parameters and loss term weights, including variable parameter and its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ae78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param dictionary\n",
    "PARAM_DICT = {\n",
    "    \"DAMPING\": [0.00015, 0.0015, 0.015, 0.15, 1.5],\n",
    "    \"INERTIA\": [0.01, 0.1325, 0.250, 0.3775, 0.5],\n",
    "    \"MECHANICAL_POWER\": [0.0, 0.0475, 0.095 , 0.1425, 0.19]\n",
    "}\n",
    "\n",
    "# Specify the range of parameter values\n",
    "PARAM_LIST: list[float] = PARAM_DICT[VARYING_PARAM.upper()]\n",
    "\n",
    "DAMPING: torch.Tensor = torch.tensor(data=[[0.15]])\n",
    "INERTIA: torch.Tensor = torch.tensor(data=[[0.25]])\n",
    "# MECHANICAL_POWER: torch.Tensor = torch.tensor(data=[[0.13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d5158",
   "metadata": {},
   "source": [
    "### 1.3) Create subdirectories required for data export (continue if already created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure desired folder has all intended subdirectories to store the results in\n",
    "parent_dir = f\"{PATH_TO_IM_DIR}/vanilla_nn/\"\n",
    "\n",
    "# Define all subdirectories to be created\n",
    "required_subdirs = [\n",
    "    \"l2_errors\",\n",
    "    \"nn_pred_vs_rk45\",\n",
    "    \"rel_error_per_epoch\",\n",
    "    \"training_loss\",\n",
    "    \"gradient_norms\",\n",
    "    \"l2_errors/semi_supervised\"\n",
    "]\n",
    "\n",
    "# Create subdirectories if not existent, else continue\n",
    "for subdir in required_subdirs:\n",
    "    full_path = os.path.join(parent_dir, subdir)\n",
    "    if not os.path.exists(full_path):\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        print(f\"Created: {full_path}\")\n",
    "    else:\n",
    "        print(f\"Already exists: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config matplotlib and define plot constants\n",
    "plt.style.use(\"science\")\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "\n",
    "CMAP: str = \"seismic\"  # Color map for the visualisations\n",
    "\n",
    "# Move tensors and models to GPU\n",
    "DEVICE: str = \"cpu\"\n",
    "\n",
    "# Print the training loss every specified number of epochs\n",
    "PRINT_TRAINING_LOSS_EVERY_EPOCH: int = 1_000\n",
    "\n",
    "# Define the parameters for the ODE numerical solution\n",
    "INITIAL_STATE: torch.tensor = torch.tensor(\n",
    "    data=np.array([0.1, 0.1]), dtype=torch.float64\n",
    ").to(device=DEVICE)\n",
    "\n",
    "# Boolean constant for whether or not PI controllers included\n",
    "CONTROLLERS: bool = False\n",
    "\n",
    "# PINN Hyperparameter constants\n",
    "LEARNING_RATE: float = 0.01\n",
    "LEARNING_RATE_LBFGS: float = 1.0\n",
    "\n",
    "# SCHEDULER_STEP_SIZE: int = 200\n",
    "PATIENCE: int = 10\n",
    "SCHEDULER_FACTOR: float = 0.9\n",
    "HISTORY: int = 100\n",
    "\n",
    "EPOCHS: int = 15_000\n",
    "EPOCHS_ADAM = 10_000\n",
    "N_C: int = 1_000  # Number of collocation points\n",
    "\n",
    "# Specify activation function\n",
    "ACTIVATION: str = \"tanh\"\n",
    "\n",
    "# Specify the number of hidden units per layer\n",
    "HIDDEN_UNITS: int = 10\n",
    "print(f\"Number of hidden units per layer: {HIDDEN_UNITS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_solutions_exact = []\n",
    "numerical_solutions_noisy = []\n",
    "numerical_domain = None\n",
    "\n",
    "# Use TIMESTEP_FLOAT instead of TIMESTEP tensor due to floating-point errors\n",
    "TIMESTEP_FLOAT = 0.1\n",
    "\n",
    "# Define number of total data points from numerical solution, N\n",
    "N: int = int((FINALTIME - T0)/(TIMESTEP_FLOAT) + 1)\n",
    "print(f\"Total number of data points from RK45, N = {N}\")\n",
    "\n",
    "# Define percentage of total dataset N to select for training\n",
    "TRAIN_TEST_SPLIT: float = 0.2\n",
    "print(f\"Percentage of data to be used for training: {TRAIN_TEST_SPLIT * 100}%\")\n",
    "\n",
    "for param in PARAM_LIST:\n",
    "\n",
    "    MECHANICAL_POWER: torch.Tensor = torch.tensor(data=[[param]])\n",
    "\n",
    "    solution, noisy_solution, numerical_times = swing_ODEs_solver(\n",
    "    initial_time=T0,\n",
    "    initial_state=INITIAL_STATE.detach().numpy(),\n",
    "    final_time=FINALTIME,\n",
    "    timestep=TIMESTEP_FLOAT,\n",
    "    inertia=INERTIA.item(),\n",
    "    damping=DAMPING.item(),\n",
    "    mechanical_power=MECHANICAL_POWER.item(),\n",
    "    voltage_magnitude=VOLTAGE.item(),\n",
    "    include_controllers=False,\n",
    "    voltages=np.array([VOLTAGES.item()]),\n",
    "    phase_angles=np.array([PHASE_ANGLES.item()]),\n",
    "    susceptances=np.array([SUSCEPTANCES.item()]),\n",
    "    file_name=\"test_run\",\n",
    "    save_output_to_file=False,\n",
    "    controller_proportional=0.05,\n",
    "    controller_integral=0.1\n",
    "    )\n",
    "\n",
    "    if numerical_domain is None:\n",
    "        numerical_domain = numerical_times\n",
    "\n",
    "    numerical_solutions_exact.append(solution)\n",
    "    numerical_solutions_noisy.append(noisy_solution)\n",
    "\n",
    "# Reshape numerical solutions into a rank-2 tensor of shape (No. of params values, 2, N)\n",
    "numerical_solutions_exact = np.array(numerical_solutions_exact)\n",
    "numerical_solutions_noisy = np.array(numerical_solutions_noisy)\n",
    "\n",
    "# Also create PyTorch tensors\n",
    "numerical_solutions_exact_tensor = torch.tensor(numerical_solutions_exact).to(device=DEVICE)\n",
    "numerical_solutions_noisy_tensor = torch.tensor(numerical_solutions_noisy).to(device=DEVICE)\n",
    "times_tensor = torch.tensor(numerical_domain[:, None].astype(np.float32), requires_grad=True).to(device=DEVICE)\n",
    "\n",
    "# Obtain a random array of index to index the numerical solution on to obtain training data\n",
    "# for vanilla NN\n",
    "rand_index = np.random.choice(np.arange(1, N, 1), replace=False, size=int(np.floor(N*TRAIN_TEST_SPLIT)))\n",
    "rand_index = np.append(rand_index, 0)\n",
    "\n",
    "print(f\"Number of training points: {rand_index.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0033ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.setdiff1d(np.arange(0, N, 1), rand_index)\n",
    "assert int(test_indices.shape[0] + rand_index.shape[0]) == N\n",
    "\n",
    "print(f\"Training data: {rand_index.shape[0]}\\nTest data: {N - rand_index.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59fac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subspace mesh\n",
    "LB: int = -2\n",
    "NUM_VERTICES: int = 40\n",
    "INCREMENT: float = -2*LB / NUM_VERTICES\n",
    "\n",
    "alpha_1: torch.Tensor = torch.arange(start=LB, end=-LB+INCREMENT, step=INCREMENT)\n",
    "alpha_2: torch.Tensor = torch.arange(start=LB, end=-LB+INCREMENT, step=INCREMENT)\n",
    "ALPHA_1, ALPHA_2 = np.meshgrid(alpha_1.numpy(), alpha_2.numpy())\n",
    "\n",
    "# Obtain the dimensionality of the parameter space\n",
    "TOTAL_NUM_PARAMS: int = summary(model=PINN(activation=ACTIVATION).to(device=DEVICE)).total_params\n",
    "print(f\"Total number of learnable parameters: {TOTAL_NUM_PARAMS}\\n\")\n",
    "\n",
    "# Sample Gaussian random direction vectors as in Li et al. (2018)\n",
    "direction_vec_1: torch.Tensor = torch.randn(TOTAL_NUM_PARAMS)\n",
    "\n",
    "# Make into a unit vectors\n",
    "direction_vec_1 = direction_vec_1 / torch.norm(input=direction_vec_1)\n",
    "\n",
    "direction_vec_2: torch.Tensor = torch.randn(TOTAL_NUM_PARAMS)\n",
    "\n",
    "# Use the Gram-Schmidt process to convert linearly independent vectors\n",
    "# into orthonormal vectors\n",
    "direction_vec_2 = (\n",
    "    direction_vec_2 - torch.dot(direction_vec_2, direction_vec_1) * direction_vec_1\n",
    ")\n",
    "\n",
    "# Make into unit vector\n",
    "direction_vec_2 = direction_vec_2 / torch.norm(input=direction_vec_2)\n",
    "\n",
    "# Test orthogonality and normality\n",
    "assert torch.dot(direction_vec_1, direction_vec_2) < 1e-6\n",
    "assert torch.norm(input=direction_vec_1) - 1 < 1e-8\n",
    "assert torch.norm(input=direction_vec_2) - 1 < 1e-8\n",
    "\n",
    "print(torch.sum(direction_vec_1))\n",
    "print(torch.sum(direction_vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e9047",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_MSE =torch.nn.MSELoss()\n",
    "\n",
    "# Collect all trained PINN models for each parameter\n",
    "nn_models = []\n",
    "\n",
    "# Collect training losses across each parameter value\n",
    "training_losses_per_param = []\n",
    "\n",
    "# Collect test losses per epoch across each parameter\n",
    "test_losses_per_param_per_epoch = []\n",
    "\n",
    "# Collect 2-norm of gradients of loss wrt weights/biases for each parameter per epoch\n",
    "gradient_norm_per_param = []\n",
    "\n",
    "# Iterate over all parameter values\n",
    "for index, param in enumerate(PARAM_LIST):\n",
    "\n",
    "    set_global_seed(SEED)\n",
    "\n",
    "    print(f\"Param value {index+1}\")\n",
    "    print(\"----------------------\\n\")\n",
    "\n",
    "    # Obtain training data\n",
    "    training_times = torch.tensor(\n",
    "        data=np.array([numerical_domain[id] for id in rand_index])[:, None],\n",
    "        requires_grad=True,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    # NB: Training data is only the phase angle\n",
    "    training_data = torch.tensor(\n",
    "        data=np.array([numerical_solutions_noisy[index, 0, id] for id in rand_index])[:, None],\n",
    "        requires_grad=False,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    test_times = torch.tensor(\n",
    "        data=np.array([numerical_domain[id] for id in test_indices])[:, None],\n",
    "        requires_grad=True,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    ground_truth_phase_angle = torch.tensor(\n",
    "        data=np.array([numerical_solutions_exact[index, 0, id] for id in test_indices])[:, None],\n",
    "        requires_grad=False,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    ground_truth_angular_frequency = torch.tensor(\n",
    "        data=np.array([numerical_solutions_exact[index, 1, id] for id in test_indices])[:, None],\n",
    "        requires_grad=False,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    label_vector = torch.stack(tensors=(ground_truth_phase_angle, ground_truth_angular_frequency), dim=0)\n",
    "\n",
    "    # Define PINN, optimiser and learning rate scheduler\n",
    "    nn = PINN(activation=ACTIVATION).to(device=DEVICE)\n",
    "\n",
    "    total_sum = sum(p.sum().item() for p in nn.parameters())\n",
    "    print(f\"Sum of all weights: {total_sum:.6f}\")\n",
    "\n",
    "    # Instantiate the Adam optimiser and learning rate scheduler\n",
    "    optimiser_adam = torch.optim.Adam(params=nn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=optimiser_adam, patience=PATIENCE, factor=SCHEDULER_FACTOR\n",
    "    )\n",
    "\n",
    "    optimiser_lbfgs = torch.optim.LBFGS(\n",
    "        params=nn.parameters(),\n",
    "        lr=LEARNING_RATE_LBFGS,\n",
    "        history_size=HISTORY,\n",
    "        line_search_fn=\"strong_wolfe\",\n",
    "        max_iter=100_000,\n",
    "        max_eval=100_000,\n",
    "        tolerance_change= np.finfo(float).eps\n",
    "    )\n",
    "\n",
    "    def closure():\n",
    "        optimiser_lbfgs.zero_grad()\n",
    "        \n",
    "        phase_angle_pred = nn.forward(\n",
    "            data=training_times, initial_state=INITIAL_STATE\n",
    "        )\n",
    "\n",
    "        loss = loss_MSE(phase_angle_pred, training_data)      \n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Define array to collect training loss every epoch\n",
    "    training_loss = []\n",
    "\n",
    "    # Define list to collect test losses every epoch (L2 relative error)\n",
    "    test_loss = []\n",
    "\n",
    "    # Define list to collect 2-norm of the gradient of the loss wrt the params every epoch\n",
    "    # ADAM only\n",
    "    gradient_norm = []\n",
    "\n",
    "    # Perform the training loop using full-batch training\n",
    "    for epoch in (range(1, EPOCHS+1)):\n",
    "\n",
    "        nn.eval()\n",
    "        phase_angle_pred_test = nn.forward(data=test_times, initial_state=INITIAL_STATE)\n",
    "        \n",
    "        nn.train()\n",
    "        angular_frequency_pred_test = torch.autograd.grad(\n",
    "            outputs=phase_angle_pred_test,\n",
    "            inputs=test_times,\n",
    "            grad_outputs=torch.ones_like(phase_angle_pred_test),\n",
    "            create_graph=False,\n",
    "            retain_graph=False\n",
    "        )[0]\n",
    "\n",
    "        pred_vector = torch.stack(tensors=(phase_angle_pred_test, angular_frequency_pred_test), dim=0)\n",
    "\n",
    "        _, l2_rel = l2_error(pred=pred_vector, ground_truth=label_vector)\n",
    "        test_loss.append(l2_rel.item())\n",
    "\n",
    "        # Obtain losses\n",
    "        phase_angle_pred = nn.forward(\n",
    "            data=training_times, initial_state=INITIAL_STATE\n",
    "        )\n",
    "\n",
    "        tot_loss = loss_MSE(phase_angle_pred, training_data)      \n",
    "        training_loss.append(tot_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            print(f\"Initial training loss: {tot_loss.item()}\")\n",
    "\n",
    "        # Backpropogate using reverse/backward-mode AD\n",
    "        if epoch <= EPOCHS_ADAM:\n",
    "            optimiser_adam.zero_grad()\n",
    "            tot_loss.backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            total_num_params = 0\n",
    "            for w_n_b in nn.parameters():\n",
    "                if w_n_b.grad is not None:\n",
    "                    total_num_params += w_n_b.flatten().shape[0]\n",
    "                    param_norm = w_n_b.grad.detach().norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            gradient_norm.append(total_norm)\n",
    "\n",
    "            assert total_num_params == TOTAL_NUM_PARAMS\n",
    "\n",
    "            optimiser_adam.step()\n",
    "            lr_scheduler.step(metrics=tot_loss)\n",
    "\n",
    "            if epoch % PRINT_TRAINING_LOSS_EVERY_EPOCH == 0:\n",
    "                print(f\"{VARYING_PARAM_NOTATION}={param:.2e}\\t\\tEpoch: {epoch}\\t\\tTraining loss: {tot_loss.item()}\")\n",
    "        else:\n",
    "            optimiser_lbfgs.step(closure=closure)\n",
    "            if epoch % PRINT_TRAINING_LOSS_EVERY_EPOCH == 0:\n",
    "                print(f\"{VARYING_PARAM_NOTATION}={param:.2e}\\t\\tEpoch: {epoch}\\t\\tRunning L-BFGS\")\n",
    "\n",
    "    # Append the trained model and the training losses per epoch to the respective lists\n",
    "    nn_models.append(nn)\n",
    "    training_losses_per_param.append(training_loss)\n",
    "    test_losses_per_param_per_epoch.append(test_loss)\n",
    "    gradient_norm_per_param.append(gradient_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the loss landscapes for both random and Hessian directions for each parameter value\n",
    "loss_landscapes_random = []\n",
    "\n",
    "# Iterate over all parameter values\n",
    "for index, (nn, param) in enumerate(zip(nn_models, PARAM_LIST)):\n",
    "\n",
    "    print(f\"Param value {index+1}\")\n",
    "    print(\"----------------------\\n\")\n",
    "\n",
    "    MECHANICAL_POWER: torch.Tensor = torch.tensor(data=[[param]])\n",
    "\n",
    "    # Place nn in evaluation mode\n",
    "    nn.eval()\n",
    "    \n",
    "    # Extract the post-training parameters\n",
    "    MINIMISER = parameters_to_vector(nn.parameters()).detach()\n",
    "\n",
    "    # Get training data\n",
    "    training_data = torch.tensor(\n",
    "        data=np.array([numerical_solutions_noisy[index, 0, id] for id in rand_index])[:, None],\n",
    "        requires_grad=False,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    training_times = torch.tensor(\n",
    "        data=np.array([numerical_domain[id] for id in rand_index])[:, None],\n",
    "        requires_grad=True,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    # Define lists to collect the loss landscapes for random directions\n",
    "    loss_landscape_random = []\n",
    "\n",
    "    # Loop over the mesh\n",
    "    for i in alpha_1:\n",
    "        for j in alpha_2:\n",
    "\n",
    "            # Perturb minimiser in the random direction subspace\n",
    "            perturbation = MINIMISER + (i * direction_vec_1) + (j * direction_vec_2)\n",
    "\n",
    "            vector_to_parameters(vec=perturbation, parameters=nn.parameters())\n",
    "\n",
    "            # Obtain losses\n",
    "            phase_angle_pred = nn.forward(\n",
    "                data=training_times, initial_state=INITIAL_STATE\n",
    "            )\n",
    "\n",
    "            loss_perturb_random = loss_MSE(phase_angle_pred, training_data)\n",
    "            loss_landscape_random.append(loss_perturb_random.detach().numpy())\n",
    "\n",
    "            # Reset PINN learnt parameters back to the minimiser\n",
    "            vector_to_parameters(vec=MINIMISER, parameters=nn.parameters())\n",
    "\n",
    "    loss_landscape_random = np.array(loss_landscape_random).reshape(alpha_1.shape[0], alpha_2.shape[0])\n",
    "    loss_landscapes_random.append(loss_landscape_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the loss landscapes for Hessian directions for each parameter value\n",
    "loss_landscapes_hessian = []\n",
    "\n",
    "# Iterate over all parameter values\n",
    "for index, (nn, param) in enumerate(zip(nn_models, PARAM_LIST)):\n",
    "\n",
    "    print(f\"Param value {index+1}\")\n",
    "    print(\"----------------------\\n\")\n",
    "\n",
    "    MECHANICAL_POWER: torch.Tensor = torch.tensor(data=[[param]])\n",
    "\n",
    "    # Place PINN in evaluation mode\n",
    "    nn.eval()\n",
    "    \n",
    "    # Extract the post-training parameters\n",
    "    MINIMISER = parameters_to_vector(nn.parameters()).detach()\n",
    "\n",
    "    # Get training data\n",
    "    training_data = torch.tensor(\n",
    "        data=np.array([numerical_solutions_noisy[index, 0, id] for id in rand_index])[:, None],\n",
    "        requires_grad=False,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    training_times = torch.tensor(\n",
    "        data=np.array([numerical_domain[id] for id in rand_index])[:, None],\n",
    "        requires_grad=True,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    # Obtain the eigenvectors of the Hessian matrix of the loss function    \n",
    "    hessian_comp = hessian(nn, loss_MSE, data=(training_times, training_data), cuda=False)\n",
    "    top_eigenvalues, top_eigenvectors = hessian_comp.eigenvalues(top_n=2)\n",
    "    \n",
    "    first_eigenvector = [t.squeeze().flatten() for t in top_eigenvectors[0]]\n",
    "    first_eigenvector = torch.cat(first_eigenvector, dim=0)\n",
    "\n",
    "    second_eigenvector = [t.squeeze().flatten() for t in top_eigenvectors[1]]\n",
    "    second_eigenvector = torch.cat(second_eigenvector, dim=0)\n",
    "\n",
    "    # Define lists to collect the loss landscapes for eigenvector directions\n",
    "    loss_landscape_hessian = []\n",
    "\n",
    "    # Loop over the mesh\n",
    "    for i in alpha_1:\n",
    "        for j in alpha_2:\n",
    "\n",
    "            perturbation_hessian = MINIMISER + (i * first_eigenvector) + (j * second_eigenvector)\n",
    "\n",
    "            vector_to_parameters(vec=perturbation_hessian, parameters=nn.parameters())\n",
    "\n",
    "            # Obtain losses\n",
    "            phase_angle_pred = nn.forward(\n",
    "                data=training_times, initial_state=INITIAL_STATE\n",
    "            )\n",
    "\n",
    "            loss_perturb_hessian = loss_MSE(phase_angle_pred, training_data)\n",
    "            loss_landscape_hessian.append(loss_perturb_hessian.detach().numpy())\n",
    "\n",
    "            # Reset PINN learnt parameters back to the minimiser\n",
    "            vector_to_parameters(vec=MINIMISER, parameters=nn.parameters())\n",
    "\n",
    "    loss_landscape_hessian = np.array(loss_landscape_hessian).reshape(alpha_1.shape[0], alpha_2.shape[0])\n",
    "    loss_landscapes_hessian.append(loss_landscape_hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_LOSS_INDEX: int = NUM_VERTICES//2\n",
    "\n",
    "# assert ALPHA_1[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX].item() == 0\n",
    "# assert ALPHA_2[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX].item() == 0\n",
    "\n",
    "# First check if the values of the loss in both loss landscapes at alpha_1 = alpha_2 = 0 are equal\n",
    "for loss, random, hessian_ in zip(training_losses_per_param, loss_landscapes_random, loss_landscapes_hessian):\n",
    "\n",
    "    print(loss[-1])\n",
    "    print(random[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX])\n",
    "    print(hessian_[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX])\n",
    "\n",
    "    assert np.isclose(random[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX], hessian_[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX], atol=1e-8, rtol=1e-8)\n",
    "    assert np.isclose(random[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX], loss[-1], atol=1e-6, rtol=1e-6)\n",
    "    assert np.isclose(hessian_[ORIG_LOSS_INDEX][ORIG_LOSS_INDEX], loss[-1], atol=1e-6, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5de2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, len(PARAM_LIST), figsize=(18, 16), subplot_kw={'projection': '3d'})\n",
    "\n",
    "# # Plot the random direction vector subspace projection of the loss landscape\n",
    "# for idx, param in enumerate(PARAM_LIST):\n",
    "#     surf_random = ax[idx].plot_surface(\n",
    "#         ALPHA_1,\n",
    "#         ALPHA_2,\n",
    "#         (loss_landscapes_random[idx]),\n",
    "#         cmap=CMAP,\n",
    "#         linewidth=0,\n",
    "#         antialiased=True\n",
    "#     )\n",
    "    \n",
    "#     clr_bar1 = fig.colorbar(surf_random, ax=ax[idx], shrink=0.07, orientation='vertical', pad=0.15)\n",
    "\n",
    "#     if idx == len(PARAM_LIST) - 1:\n",
    "#         clr_bar1.set_label(\n",
    "#             r\"$\\mathcal{F}(\\alpha_1, \\alpha_2)$\",\n",
    "#             fontsize=15,\n",
    "#             rotation=-90,\n",
    "#             labelpad=25\n",
    "#         )\n",
    "\n",
    "#     ax[idx].set_xlabel(r'$\\alpha_1$', fontsize=15, labelpad=1.1)\n",
    "#     ax[idx].set_ylabel(r'$\\alpha_2$', fontsize=15)\n",
    "\n",
    "#     ax[idx].set_title(f'{VARYING_PARAM_NOTATION}='+f\"{param:.1e}\", fontsize=15)\n",
    "\n",
    "# fig.subplots_adjust(left=0.1, right=0.9, hspace=-0.85, top=1.3)\n",
    "\n",
    "# # Output the plots as PDF to the desired directory\n",
    "# loss_landscape_image_name_random = f\"vanilla_nn/loss_landscape_random_seed_{SEED}.pdf\"\n",
    "\n",
    "# if SAVE:\n",
    "#     print(\"Saving...\")\n",
    "#     plt.savefig(fname=PATH_TO_IM_DIR+loss_landscape_image_name_random, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f50a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, len(PARAM_LIST), figsize=(18, 15), subplot_kw={'projection': '3d'})\n",
    "\n",
    "# Plot the random direction vector subspace projection of the loss landscape\n",
    "for idx, param in enumerate(PARAM_LIST):\n",
    "    surf_random = ax[0, idx].plot_surface(\n",
    "        ALPHA_1,\n",
    "        ALPHA_2,\n",
    "        (loss_landscapes_random[idx]),\n",
    "        cmap=CMAP,\n",
    "        linewidth=0,\n",
    "        antialiased=True\n",
    "    )\n",
    "    \n",
    "    clr_bar1 = fig.colorbar(surf_random, ax=ax[0, idx], shrink=0.10, orientation='vertical', pad=0.15)\n",
    "\n",
    "    if idx == len(PARAM_LIST) - 1:\n",
    "        clr_bar1.set_label(\n",
    "            r\"$\\mathcal{F}(\\alpha_1, \\alpha_2)$\",\n",
    "            fontsize=14,\n",
    "            rotation=-90,\n",
    "            labelpad=25\n",
    "        )\n",
    "\n",
    "    ax[0, idx].set_xlabel(r'$\\alpha_1$', fontsize=14)\n",
    "    ax[0, idx].set_ylabel(r'$\\alpha_2$', fontsize=14)\n",
    "\n",
    "    if TOP_PLOT:\n",
    "        ax[0, idx].set_title(f'{VARYING_PARAM_NOTATION}='+f\"{param:.3e}\", fontsize=15)\n",
    "\n",
    "    # Plot the Hessian eigenvectors subspace projection of the loss landscape\n",
    "    surf_hessian = ax[1, idx].plot_surface(\n",
    "        ALPHA_1,\n",
    "        ALPHA_2,\n",
    "        (loss_landscapes_hessian[idx]),\n",
    "        cmap=CMAP,\n",
    "        # vmin=vmin,\n",
    "        # vmax=vmax,\n",
    "        linewidth=0,\n",
    "        antialiased=True\n",
    "    )\n",
    "\n",
    "    clr_bar2 = fig.colorbar(surf_hessian, ax=ax[1, idx], shrink=0.10, orientation='vertical', pad=0.2)\n",
    "\n",
    "    if idx == len(PARAM_LIST) - 1:\n",
    "        clr_bar2.set_label(\n",
    "            r\"$\\mathcal{F}(\\alpha_1, \\alpha_2)$\",\n",
    "            fontsize=15,\n",
    "            rotation=-90,\n",
    "            labelpad=25\n",
    "        )\n",
    "    \n",
    "    ax[1, idx].set_xlabel(r'$\\alpha_1$', fontsize=15, labelpad=1.1)\n",
    "    ax[1, idx].set_ylabel(r'$\\alpha_2$', fontsize=15)\n",
    "    # ax[1, idx].set_zlim(vmin, vmax)\n",
    "\n",
    "fig.subplots_adjust(left=0.1, right=0.9, hspace=-0.85, top=1.3)\n",
    "\n",
    "# Output the plots as PDF to the desired directory\n",
    "loss_landscape_image_name = f\"vanilla_nn/loss_landscape_random_seed_{SEED}.pdf\"\n",
    "\n",
    "if SAVE is True and SEMI_SUPERVISED is False:\n",
    "    print(\"Saving...\")\n",
    "    plt.savefig(fname=PATH_TO_IM_DIR+loss_landscape_image_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, len(PARAM_LIST), figsize=(20, 8), sharex=True, sharey=True)\n",
    "\n",
    "num_levels = 100\n",
    "# levels = np.linspace(vmin, vmax, num_levels)\n",
    "\n",
    "for index, (param) in enumerate(PARAM_LIST):\n",
    "    contour_random = ax[0, index].contourf(\n",
    "        ALPHA_1,\n",
    "        ALPHA_2,\n",
    "        (loss_landscapes_random[index]),\n",
    "        cmap=CMAP,\n",
    "        levels=num_levels,\n",
    "        antialiased=True\n",
    "    )\n",
    "\n",
    "    clr_bar1 = fig.colorbar(contour_random, ax=ax[0, index], shrink=1, orientation='vertical', pad=0.05)\n",
    "\n",
    "    if index == len(PARAM_LIST) - 1:\n",
    "        clr_bar1.set_label(\n",
    "            r\"$\\mathcal{F}(\\alpha_1, \\alpha_2)$\",\n",
    "            fontsize=15,\n",
    "            rotation=-90,\n",
    "            labelpad=25\n",
    "        )\n",
    "    \n",
    "    if index == 0:\n",
    "        ax[0, index].set_ylabel(r'$\\alpha_2$', fontsize=17)\n",
    "        ax[1, index].set_ylabel(r'$\\alpha_2$', fontsize=17)\n",
    "\n",
    "    if TOP_PLOT:\n",
    "        ax[0, index].set_title(f'{VARYING_PARAM_NOTATION}='+f\"{param:.3e}\", fontsize=15)\n",
    "\n",
    "\n",
    "    contour_hessian = ax[1, index].contourf(\n",
    "        ALPHA_1,\n",
    "        ALPHA_2,\n",
    "        (loss_landscapes_hessian[index]),\n",
    "        cmap=CMAP,\n",
    "        levels=num_levels,\n",
    "        antialiased=True\n",
    "    )\n",
    "\n",
    "    clr_bar2 = fig.colorbar(contour_hessian, ax=ax[1, index], shrink=1, orientation='vertical', pad=0.05)\n",
    "\n",
    "    if index == len(PARAM_LIST) - 1:\n",
    "        clr_bar2.set_label(\n",
    "            r\"$\\mathcal{F}(\\alpha_1, \\alpha_2)$\",\n",
    "            fontsize=15,\n",
    "            rotation=-90,\n",
    "            labelpad=25\n",
    "        )\n",
    "    \n",
    "    ax[1, index].set_xlabel(r'$\\alpha_1$', fontsize=15)\n",
    "\n",
    "# Shared colorbar\n",
    "# cbar = fig.colorbar(contours_pinn[0], ax=ax.ravel().tolist(), shrink=1, orientation='vertical', pad=0.01)\n",
    "\n",
    "contour_image_name = f\"vanilla_nn/contour_random_seed_{SEED}.pdf\"\n",
    "\n",
    "if SAVE is True and SEMI_SUPERVISED is False:\n",
    "    print(\"Saving...\")\n",
    "    plt.savefig(fname=PATH_TO_IM_DIR+contour_image_name, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114afec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_errors_pinns = []\n",
    "rel_errors_pinns = []\n",
    "pinn_predictions_per_param = []\n",
    "\n",
    "fig, ax = plt.subplots(2, len(PARAM_LIST), figsize=(17, 10), sharex=True)\n",
    "\n",
    "for index, (model, param) in enumerate(zip(nn_models, PARAM_LIST)):\n",
    "\n",
    "    test_times = torch.tensor(\n",
    "        data=np.array([numerical_domain[id] for id in test_indices])[:, None],\n",
    "        requires_grad=True,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    ground_truth_phase_angle = torch.tensor(\n",
    "        data=np.array([numerical_solutions_exact[index, 0, id] for id in test_indices])[:, None],\n",
    "        requires_grad=False,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    ground_truth_angular_frequency = torch.tensor(\n",
    "        data=np.array([numerical_solutions_exact[index, 1, id] for id in test_indices])[:, None],\n",
    "        requires_grad=False,\n",
    "        dtype=torch.float32\n",
    "    ).to(device=DEVICE)\n",
    "\n",
    "    assert ground_truth_angular_frequency.shape[0] == int(N - TRAIN_TEST_SPLIT*N)\n",
    "    assert ground_truth_phase_angle.shape[0] == int(N - TRAIN_TEST_SPLIT*N)\n",
    "    assert test_times.shape[0] == int(N - TRAIN_TEST_SPLIT*N)\n",
    "\n",
    "    nn = model\n",
    "    nn.eval()\n",
    "    pred = nn.forward(data=test_times)\n",
    "    pred_dot = torch.autograd.grad(\n",
    "        outputs=pred,\n",
    "        inputs=test_times,\n",
    "        grad_outputs=torch.ones_like(pred),\n",
    "        create_graph=False,\n",
    "        retain_graph=False\n",
    "    )[0]\n",
    "\n",
    "    pred_vector = torch.stack(tensors=(pred, pred_dot), dim=0).squeeze()\n",
    "    label_vector = torch.stack(tensors=(ground_truth_phase_angle, ground_truth_angular_frequency), dim=0).squeeze()\n",
    "\n",
    "    l2_abs, l2_rel = l2_error(pred=pred_vector, ground_truth=label_vector)\n",
    "\n",
    "    print(f\"L2 absolute error: {l2_abs:.2e}\")\n",
    "    print(f\"L2 relative error: {l2_rel:.2e}\")\n",
    "\n",
    "    abs_errors_pinns.append(l2_abs)\n",
    "    rel_errors_pinns.append(l2_rel)\n",
    "    pinn_predictions_per_param.append(pred_vector.detach().numpy())\n",
    "\n",
    "    ax[0, index].plot(test_times.detach().numpy(), ground_truth_phase_angle.detach().numpy(), color=\"black\", linestyle=\"-\", label=\"True dynamics\")\n",
    "    ax[0, index].plot(test_times.detach().numpy(), pred.detach().numpy(), color='red', linestyle='--', label='Vanilla NN prediction', linewidth=1.5)\n",
    "    ax[0, index].grid()\n",
    "    # ax[0, index].legend(fontsize=12, loc=\"best\")\n",
    "    ax[0, index].set_title(f'{VARYING_PARAM_NOTATION}='+f\"{param:.3e}\", fontsize=14)\n",
    "\n",
    "    ax[1, index].plot(test_times.detach().numpy(), ground_truth_angular_frequency.detach().numpy(), color=\"black\", linestyle=\"-\", label=\"True dynamics\")\n",
    "    ax[1, index].plot(test_times.detach().numpy(), pred_dot.detach().numpy(), color='red', linestyle='--', label='Vanilla NN prediction', linewidth=1.5)\n",
    "    ax[1, index].grid()\n",
    "    # ax[1, index].legend(fontsize=12, loc=\"best\")\n",
    "    ax[1, index].set_xlabel(\"Time (s)\", fontsize=14)\n",
    "\n",
    "    if index == 0:\n",
    "        ax[0, index].set_ylabel(\"Phase Angle $\\delta$ (rad)\", fontsize=14)\n",
    "        ax[1, index].set_ylabel(\"Angular Frequency $\\dot{\\delta}$ (rad/s)\", fontsize=14)\n",
    "\n",
    "exact_vs_pred_image: str = f\"vanilla_nn/vanilla_nn_vs_ground_truth_seed_{SEED}.pdf\"\n",
    "\n",
    "# plt.subplots_adjust(left=0.1, right=0.9)\n",
    "# plt.tight_layout()\n",
    "\n",
    "handles, labels = ax[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.02), fontsize=14)\n",
    "\n",
    "if SAVE is True and SEMI_SUPERVISED is False:\n",
    "    print(\"Saving...\")\n",
    "    plt.savefig(fname=PATH_TO_IM_DIR+exact_vs_pred_image, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "ax[0].loglog(PARAM_LIST, [ls.detach().numpy() for ls in abs_errors_pinns], marker='s', linestyle='--', label=\"NN\")    \n",
    "ax[0].set_xlabel(f\"{VARYING_PARAM.title()} (${VARYING_PARAM_NOTATION}$)\", fontsize=13)\n",
    "ax[0].set_ylabel(\"$L_{2}$ Absolute Error\", fontsize=13)\n",
    "ax[0].legend(fontsize=12)\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].loglog(PARAM_LIST, [ls.detach().numpy() for ls in rel_errors_pinns], marker='s', linestyle='--', label=\"NN\")    \n",
    "ax[1].set_xlabel(f\"{VARYING_PARAM.title()} (${VARYING_PARAM_NOTATION}$)\", fontsize=13)\n",
    "ax[1].set_ylabel(\"$L_{2}$ Relative Error\", fontsize=13)\n",
    "# ax[1].set_xticklabels()\n",
    "ax[1].legend(fontsize=12)\n",
    "ax[1].grid()\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.9, hspace=-0.7)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "for (param, training_loss) in zip(PARAM_LIST, training_losses_per_param):\n",
    "    ax.semilogy(\n",
    "        range(1, EPOCHS+1),\n",
    "        training_loss,\n",
    "        alpha=0.7,\n",
    "        label=f\"{VARYING_PARAM_NOTATION}={param:.3e}\"\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Training loss $\\mathcal{L}_{\\mathrm{PINN}}$\", fontsize=12)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=12)\n",
    "ax.grid()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=len(PARAM_LIST), bbox_to_anchor=(0.525, -0.08), fontsize=15)\n",
    "\n",
    "training_losses_plot = f\"{VARYING_PARAM}_training_losses_seed_{SEED}.pdf\"\n",
    "\n",
    "# if SAVE:\n",
    "#     plt.savefig(fname=PATH_TO_IM_DIR+training_losses_plot, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e98a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "for param, grad_norm in zip(PARAM_LIST, gradient_norm_per_param):\n",
    "    ax.semilogx(\n",
    "        range(1, EPOCHS_ADAM+1),\n",
    "        grad_norm,\n",
    "        label=f\"{VARYING_PARAM_NOTATION}={param:.3e}\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(r\"Gradient Norm $\\lVert \\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}\\rVert_{2}$\", fontsize=15)\n",
    "ax.set_xlabel(\"Epochs (Adam)\", fontsize=15)\n",
    "ax.grid()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=len(PARAM_LIST), bbox_to_anchor=(0.525, -0.08), fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE is True and SEMI_SUPERVISED is True:\n",
    "    abs_errors_pinns_export = [err.item() for err in abs_errors_pinns]\n",
    "    rel_errors_pinns_export = [err.item() for err in rel_errors_pinns]\n",
    "\n",
    "    errors_output = f\"vanilla_nn/l2_errors/semi_supervised/semi_supervised_abs_rel_errors_seed_{SEED}_Nd_{rand_index.shape[0]}.npz\"\n",
    "    \n",
    "    np.savez(PATH_TO_IM_DIR+errors_output, absolute_errors=abs_errors_pinns_export, relative_errors=rel_errors_pinns_export)\n",
    "\n",
    "elif SAVE is True and SEMI_SUPERVISED is False:\n",
    "    abs_errors_pinns_export = [err.item() for err in abs_errors_pinns]\n",
    "    rel_errors_pinns_export = [err.item() for err in rel_errors_pinns]\n",
    "\n",
    "    errors_output = f\"vanilla_nn/l2_errors/abs_rel_errors_seed_{SEED}.npz\"\n",
    "    \n",
    "    np.savez(PATH_TO_IM_DIR+errors_output, absolute_errors=abs_errors_pinns_export, relative_errors=rel_errors_pinns_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_output = f\"vanilla_nn/training_loss/training_loss_per_epoch_seed_{SEED}.npz\"\n",
    "\n",
    "if SAVE is True and SEMI_SUPERVISED is False:\n",
    "    np.savez(\n",
    "        PATH_TO_IM_DIR+training_loss_output,\n",
    "        total_loss=np.array(training_losses_per_param)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_output = f\"vanilla_nn/rel_error_per_epoch/rel_errors_per_epoch_seed_{SEED}.npz\"\n",
    "\n",
    "if SAVE is True and SEMI_SUPERVISED is False:\n",
    "    np.savez(\n",
    "        PATH_TO_IM_DIR+test_loss_output,\n",
    "        test_loss=np.array(test_losses_per_param_per_epoch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b896a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_times = torch.tensor(\n",
    "    data=np.array([numerical_domain[id] for id in test_indices])[:, None],\n",
    "    requires_grad=True,\n",
    "    dtype=torch.float32\n",
    ").to(device=DEVICE)\n",
    "\n",
    "all_pa = torch.tensor(\n",
    "    data=np.array([numerical_solutions_exact[:, 0, id] for id in test_indices])[:, None],\n",
    "    requires_grad=False,\n",
    "    dtype=torch.float32\n",
    ").to(device=DEVICE)\n",
    "\n",
    "all_af = torch.tensor(\n",
    "    data=np.array([numerical_solutions_exact[:, 1, id] for id in test_indices])[:, None],\n",
    "    requires_grad=False,\n",
    "    dtype=torch.float32\n",
    ").to(device=DEVICE)\n",
    "\n",
    "print(np.array(pinn_predictions_per_param)[:, 0, :].shape)\n",
    "print(np.array(all_pa).squeeze().T.shape)\n",
    "print(test_times.detach().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72eb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_predictions_vs_rk45 = f\"vanilla_nn/nn_pred_vs_rk45/vanilla_nn_pred_vs_rk45_seed_{SEED}.npz\"\n",
    "\n",
    "if SAVE is True and SEMI_SUPERVISED is False:\n",
    "    np.savez(\n",
    "        PATH_TO_IM_DIR+pinn_predictions_vs_rk45,\n",
    "        pred=np.array(pinn_predictions_per_param)[:, 0, :],\n",
    "        pred_dot=np.array(pinn_predictions_per_param)[:, 1, :],\n",
    "        phase_angle_label = np.array(all_pa).squeeze().T,\n",
    "        angular_frequency_label = np.array(all_af).squeeze().T,\n",
    "        test_times = test_times.detach().numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad_norms = f\"vanilla_nn/gradient_norms/loss_grad_norm_per_epoch_seed_{SEED}.npz\"\n",
    "\n",
    "if SAVE is True and SEMI_SUPERVISED is False:\n",
    "    np.savez(\n",
    "        PATH_TO_IM_DIR+loss_grad_norms,\n",
    "        grad_norms=np.array(gradient_norm_per_param)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892246e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
